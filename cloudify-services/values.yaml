imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""
ipv6_enabled: false

serviceAccount:
  create: true
  annotations: { }
  name: ""

service:
  type: ClusterIP
  port: 80
  http:
    port: 80

ingress:
  enabled: true
  host:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 50m
  tls:
    enabled: false
    secretName: cfy-secret-name

certs:
  ca_cert: ""
  ca_key: ""
  external_cert: ""
  external_key: ""
  internal_cert: ""
  internal_key: ""
  rabbitmq_cert: ""
  rabbitmq_key: ""

resources:
  packages:
    agents:
      manylinux-x86_64-agent.tar.gz: https://cloudify-release-eu.s3.amazonaws.com/cloudify/7.0.0/ga-release/manylinux-x86_64-agent_7.0.0-ga.tar.gz
      manylinux-aarch64-agent.tar.gz: https://cloudify-release-eu.s3.amazonaws.com/cloudify/7.0.0/ga-release/manylinux-aarch64-agent_7.0.0-ga.tar.gz
      cloudify-windows-agent.exe: https://cloudify-release-eu.s3.amazonaws.com/cloudify/7.0.0/ga-release/cloudify-windows-agent_7.0.0-ga.exe

composer_backend:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-composer-backend:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 3000
  probes:
    liveness:
      enabled: false
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  securityContext:
    runAsNonRoot: true

composer_frontend:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-composer-frontend:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 8188
  probes:
    liveness:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  securityContext:
    runAsNonRoot: true

execution_scheduler:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-execution-scheduler:latest-x86_64
  imagePullPolicy: IfNotPresent

mgmtworker:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-mgmtworker:latest-x86_64
  imagePullPolicy: IfNotPresent
  access:
    local_cluster: true

nginx:
  replicas: 1
  image: docker.io/library/nginx
  imagePullPolicy: IfNotPresent
  http_port: 80
  https_port: 443
  internal_port: 53333
  probes:
    liveness:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  # configure request rate-limits. If enabled, requests are rate-limited based
  # on the remote IP address.
  # Requests that authenticate with a valid execution-token, are never
  # rate-limited
  rate_limit:
    enabled: true
    # rate is a string in the form of "10r/s" (10 requests per second)
    # or "600r/m" (600 requests per minute)
    rate: "10r/s"
    # burst and delay manage the request queueing mechanism. With the
    # default settings of burst=30 and delay=20, up to 30 requests
    # can be queued per IP (i.e. before nginx starts responding with 503),
    # and the first 20 requests will be served without any delay. Then, requests
    # will be delayed according to the rate, and if there's more than 30 queued
    # total, will receive 503.
    burst: 30

    delay: 20
db:
  host: "postgresql"
  dbName: &postgresqlDB "cloudify_db"
  user: &postgresqlUser "cloudify"
  password: &postgresqlPassword "cloudify"
  k8sSecret:
    name: "cloudify-db-creds"
    key: "password"


postgresql:
  enabled: true
  fullnameOverride: postgresql
  auth:
    username: *postgresqlUser
    database: *postgresqlDB
    password: *postgresqlPassword
  image:
    tag: 15.3.0-debian-11-r17
    pullPolicy: IfNotPresent
  containerPorts:
    postgresql: 5432
  primary:
    resources:
      limits:
        memory: 2Gi
        cpu: 2
      requests:
        memory: 256Mi
        cpu: 0.5
  podLabels: {}
  enableNetworkPolicy: true
  metrics:
    enabled: true

rabbitmq:
  enabled: true
  image:
    tag: 3.12.2-debian-11-r8
    pullPolicy: IfNotPresent
  fullnameOverride: rabbitmq
  plugins: "rabbitmq_management rabbitmq_prometheus rabbitmq_tracing rabbitmq_peer_discovery_k8s"
  auth:
    username: cloudify
    password: c10udify
    erlangCookie: "cloudify-erlang-cookie"
    tls:
      enabled: true
      failIfNoPeerCert: false
      existingSecret: rabbitmq-ssl-certs
  advancedConfiguration: |-
    [
      {rabbit, [
        {consumer_timeout, undefined}
      ]}
    ].
  extraConfiguration: |-
    {{ if .Values.ipv6_enabled }}
    management.ssl.ip         = ::
    {{ end }}
    management.ssl.port       = 15671
    management.ssl.cacertfile = /opt/bitnami/rabbitmq/certs/ca_certificate.pem
    management.ssl.certfile   = /opt/bitnami/rabbitmq/certs/server_certificate.pem
    management.ssl.keyfile    = /opt/bitnami/rabbitmq/certs/server_key.pem
  loadDefinition:
    enabled: true
    existingSecret: rabbitmq-load-definition
  extraSecrets:
    rabbitmq-load-definition:
      load_definition.json: |
        {
            "vhosts": [
                {
                    "name": "/"
                }
            ],
            "users": [
                {
                    "hashing_algorithm": "rabbit_password_hashing_sha256",
                    "name": "{{ .Values.auth.username }}",
                    "password": "{{ .Values.auth.password }}",
                    "tags": "administrator"
                }
            ],
            "permissions": [
                {
                    "user": "{{ .Values.auth.username }}",
                    "vhost": "/",
                    "configure": ".*",
                    "write": ".*",
                    "read": ".*"
                }
            ],
            "policies": [
                {
                    "name": "logs_queue_message_policy",
                    "vhost": "/",
                    "pattern": "^cloudify-log$",
                    "priority": 100,
                    "apply-to": "queues",
                    "definition": {
                        "message-ttl": 1200000,
                        "max-length": 1000000,
                        "ha-mode": "all",
                        "ha-sync-mode": "automatic",
                        "ha-sync-batch-size": 50
                    }
                },
                {
                    "name": "events_queue_message_policy",
                    "vhost": "/",
                    "pattern": "^cloudify-events$",
                    "priority": 100,
                    "apply-to": "queues",
                    "definition": {
                        "message-ttl": 1200000,
                        "max-length": 1000000,
                        "ha-mode": "all",
                        "ha-sync-mode": "automatic",
                        "ha-sync-batch-size": 50
                    }
                },
                {
                    "name": "default_policy",
                    "vhost": "/",
                    "pattern": "^",
                    "priority": 1,
                    "apply-to": "queues",
                    "definition": {
                        "ha-mode": "all",
                        "ha-sync-mode": "automatic",
                        "ha-sync-batch-size": 50
                    }
                }
            ],
            "queues": [
                {
                    "arguments": {},
                    "auto_delete": false,
                    "durable": true,
                    "name": "cloudify.management_operation",
                    "type": "classic",
                    "vhost": "/"
                },
                {
                    "arguments": {},
                    "auto_delete": false,
                    "durable": true,
                    "name": "cloudify.management_workflow",
                    "type": "classic",
                    "vhost": "/"
                }
            ],
            "bindings": [
                {
                    "arguments": {},
                    "destination": "cloudify.management_operation",
                    "destination_type": "queue",
                    "routing_key": "operation",
                    "source": "cloudify.management",
                    "vhost": "/"
                },
                {
                    "arguments": {},
                    "destination": "cloudify.management_workflow",
                    "destination_type": "queue",
                    "routing_key": "workflow",
                    "source": "cloudify.management",
                    "vhost": "/"
                }
            ],
            "exchanges": [
                {
                    "arguments": {},
                    "auto_delete": false,
                    "durable": true,
                    "name": "cloudify.management",
                    "type": "direct",
                    "vhost": "/"
                }
            ]
        }
  service:
    ports:
      metrics: 15692
    extraPorts:
      - name: manager-ssl
        port: 15671
        targetPort: 15671
  resources:
    requests:
      memory: 512Mi
      cpu: 0.5
    limits:
      memory: 1Gi
      cpu: 4
  podLabels: {}
  enableNetworkPolicy: true
  metrics:
    enabled: true

rest_service:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-restservice:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 8100
  probes:
    liveness:
      enabled: false
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  configPath: /tmp/config.yaml
  s3:
    credentials_secret_name: seaweedfs-s3-secret
    session_token_secret_name: ""
  config:
    manager:
      hostname: cloudify-manager
      private_ip: localhost
      public_ip: localhost
      file_server_type: s3
      s3_resources_bucket: resources
      s3_server_url: http://seaweedfs-s3.default.svc.cluster.local:8333
      prometheus_url: http://prometheus-server:9090

api_service:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-apiservice:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 8101
  probes:
    liveness:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3

stage_backend:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-stage-backend:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 8088
  probes:
    liveness:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  securityContext:
    runAsNonRoot: true

stage_frontend:
  replicas: 1
  image: 263721492972.dkr.ecr.eu-west-1.amazonaws.com/cloudify-manager-stage-frontend:latest-x86_64
  imagePullPolicy: IfNotPresent
  port: 8188
  probes:
    liveness:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 3
  securityContext:
    runAsNonRoot: true

# -- Parameters group for bitnami/prometheus helm chart.
# Details: https://github.com/bitnami/charts/blob/main/bitnami/prometheus/README.md
# @default -- object
prometheus:
  enabled: true
  fullnameOverride: prometheus
  alertmanager:
    enabled: false
  server:
    persistence:
      enabled: true
    image:
      tag: 2.45.0
      pullPolicy: IfNotPresent
    resources:
      limits:
        memory: 512Mi
        cpu: 1
      requests:
        memory: 512Mi
        cpu: 0.1
    replicaCount: 1
    service:
      ports:
        http: 9090
        type: ClusterIP
    extraScrapeConfigs:
      - job_name: kube-state-metrics
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_label_app_kubernetes_io_name
            action: keep
            regex: kube-state-metrics
      - job_name: postgresql-metrics
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_name
            action: keep
            regex: postgresql-metrics
          - source_labels:
              - __meta_kubernetes_service_cluster_ip
            target_label: host
            action: replace
      - job_name: rabbitmq-metrics
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_label_app_kubernetes_io_name
            action: keep
            regex: rabbitmq
          - source_labels:
              - __meta_kubernetes_service_port_name
            action: keep
            regex: metrics
          - source_labels:
              - __meta_kubernetes_service_cluster_ip
            target_label: host
            action: replace
    alertingRules:
      groups:
        - name: manager
          rules:
            - record: manager_service
              expr: group by(deployment, statefulset) (kube_deployment_status_replicas_ready or kube_statefulset_status_replicas_ready)
              labels:
            - record: manager_healthy
              expr: sum by (namespace) (kube_deployment_status_replicas_ready{deployment="api-service"} or kube_deployment_status_replicas_ready{deployment="rest-service"} or kube_deployment_status_replicas_ready{deployment="execution-scheduler"} or kube_deployment_status_replicas_ready{deployment="nginx"} or kube_statefulset_status_replicas_ready{statefulset="mgmtworker"}) >= 5
              labels:
            - alert: manager_down
              expr: kube_deployment_status_replicas_ready{deployment="api-service"} == 0 or absent(kube_deployment_status_replicas_ready{deployment="api-service"}) or kube_deployment_status_replicas_ready{deployment="rest-service"} == 0 or absent(kube_deployment_status_replicas_ready{deployment="rest-service"}) or kube_deployment_status_replicas_ready{deployment="execution-scheduler"} == 0 or absent(kube_deployment_status_replicas_ready{deployment="execution-scheduler"}) or kube_deployment_status_replicas_ready{deployment="nginx"} == 0 or absent(kube_deployment_status_replicas_ready{deployment="nginx"}) or kube_statefulset_status_replicas_ready{statefulset="mgmtworker"} == 0 or absent(kube_statefulset_status_replicas_ready{statefulset="mgmtworker"})
              for: 15s
              labels:
                severity: critical
              annotations:
                summary: "Manager is down"
        - name: postgresql
          rules:
            - record: postgres_healthy
              expr: pg_up{job="postgresql-metrics"} > 0 and up{job="postgresql-metrics"} > 0
              labels:
            - alert: postgres_down
              expr: pg_up{job="postgresql-metrics"} == 0 or absent(up{job="postgresql-metrics"})
              for: 15s
              labels:
                severity: critical
              annotations:
                summary: "PostgreSQL is down"
        - name: rabbitmq
          rules:
            - record: rabbitmq_healthy
              expr: sum by(host, instance, job, monitor) (up{job="rabbitmq-metrics"}) > 0
              labels:
            - alert: rabbitmq_down
              expr: up{job="rabbitmq-metrics"} == 0
              for: 15s
              labels:
                severity: critical
              annotations:
                summary: "RabbitMQ is down"
    scrapeInterval: 15s
    evaluationInterval: 15s
    # Rewriting configuration to render `rule_files:` part which otherwise requires alertmanager to be enabled.  This is how
    # the original configuration is rendered in Bitnami's Helm chart for Prometheus:
    # https://github.com/bitnami/charts/blob/c0cb65e21512743aa17ca55a7e97609d7e2b273a/bitnami/prometheus/values.yaml#L635-L646
    configuration: |
      global:
        {{- if .Values.server.scrapeInterval }}
        scrape_interval: {{ .Values.server.scrapeInterval }}
        {{- end }}
        {{- if .Values.server.scrapeTimeout }}
        scrape_timeout: {{ .Values.server.scrapeTimeout }}
        {{- end }}
        {{- if .Values.server.evaluationInterval }}
        evaluation_interval: {{ .Values.server.evaluationInterval }}
        {{- end }}
        external_labels:
          monitor: {{ template "common.names.fullname" . }}
          {{- if .Values.server.externalLabels }}
          {{- include "common.tplvalues.render" (dict "value" .Values.server.externalLabels "context" $) | nindent 4 }}
          {{- end }}
      {{- if .Values.server.remoteWrite }}
      remote_write: {{- include "common.tplvalues.render" (dict "value" .Values.server.remoteWrite "context" $) | nindent 4 }}
      {{- end }}
      scrape_configs:
        - job_name: prometheus
        {{- include "prometheus.scrape_config" (dict "component" "server" "context" $) | nindent 4 }}
      {{- if .Values.alertmanager.enabled }}
        - job_name: alertmanager
          {{- include "prometheus.scrape_config" (dict "component" "alertmanager" "context" $) | nindent 4 }}
      {{- end }}
      {{- if .Values.server.extraScrapeConfigs}}
      {{- include "common.tplvalues.render" (dict "value" .Values.server.extraScrapeConfigs "context" $) | nindent 2 }}
      {{- end }}
      {{- if or .Values.alertmanager.enabled .Values.server.alertingEndpoints}}
      alerting:
        alertmanagers:
          {{- if .Values.server.alertingEndpoints }}
          {{- include "common.tplvalues.render" (dict "value" .Values.server.alertingEndpoints "context" $) | nindent 4 }}
          {{- end }}
          - scheme: HTTP
            static_configs:
              - targets: [ "{{ printf "%s.%s.svc.%s:%d" (include "prometheus.alertmanager.fullname" .) (include "common.names.namespace" .) .Values.clusterDomain (int .Values.alertmanager.service.ports.http) }}" ]
      {{- end }}
      rule_files:
        - rules.yaml
  podLabels: {}
  enableNetworkPolicy: true

# -- Parameters group for bitnami/kube-state-metrics helm chart.
# Details: https://github.com/bitnami/charts/tree/main/bitnami/kube-state-metrics/README.md
# @default -- object
kube-state-metrics:
  enabled: true
  fullnameOverride: kube-state-metrics
  namespaces: "{{ .Release.Namespace }}"
  extraArgs:
    metric-allowlist: "kube_deployment_spec_replicas,kube_deployment_status_replicas,kube_deployment_status_replicas_ready,kube_deployment_status_replicas_unavailable,kube_statefulset_spec_replicas,kube_statefulset_status_replicas,kube_statefulset_status_replicas_ready,kube_statefulset_status_replicas_unavailable"
  kubeResources:
    deployments: true
    certificatesigningrequests: false
    configmaps: false
    cronjobs: false
    daemonsets: false
    endpoints: false
    horizontalpodautoscalers: false
    ingresses: false
    jobs: false
    limitranges: false
    mutatingwebhookconfigurations: false
    namespaces: false
    networkpolicies: false
    nodes: false
    persistentvolumeclaims: false
    persistentvolumes: false
    poddisruptionbudgets: false
    pods: false
    replicasets: false
    replicationcontrollers: false
    resourcequotas: false
    secrets: false
    services: false
    statefulsets: true
    storageclasses: false
    verticalpodautoscalers: false
    validatingwebhookconfigurations: false
    volumeattachments: false
  podLabels: {}
  enableNetworkPolicy: true

# -- Parameters group for seaweed helm chart.
# Details: https://github.com/seaweedfs/seaweedfs/tree/master/k8s/charts/seaweedfs
# @default -- object
seaweedfs:
  enabled: true
  master:
    resources: |
      requests:
        memory: 128Mi
        cpu: 0.1
      limits:
        memory: 512Mi
        cpu: 0.5
    data:
      type: "persistentVolumeClaim"
      size: "1Gi"
    affinity: ""
    nodeSelector: ""
  filer:
    resources: |
      requests:
        memory: 128Mi
        cpu: 0.1
      limits:
        memory: 512Mi
        cpu: 0.5
    data:
      type: "persistentVolumeClaim"
      size: "1Gi"
    affinity: ""
    nodeSelector: ""
  s3:
    skipAuthSecretCreation: false
    enabled: true
    enableAuth: false
    port: 8333
    resources: |
      requests:
        memory: 128Mi
        cpu: 0.1
      limits:
        memory: 512Mi
        cpu: 0.5
    affinity: ""
    nodeSelector: ""
  volume:
    maxVolumes: "50"
    resources: |
      requests:
        memory: 256Mi
        cpu: 0.1
      limits:
        memory: 512Mi
        cpu: 0.5
    data:
      type: "persistentVolumeClaim"
      size: "10Gi"
    affinity: ""
    nodeSelector: ""
  # -- Parameters group for awscli containers (using as init containers)
  # @default -- object
  clientImage: docker.io/amazon/aws-cli:2.13.9
