[//]: # (-*- markdown -*-)

# Cloudify manager worker helm chart ( Premium Version )

## Description

It's a helm chart for cloudify manager which:

- Is highly available, can be deployed with multiple replicas. ( available only when used NFS like Storage file system )
- Uses persistent volume to survive restarts/failures.
- Uses DB (PostgreSQL), which may be deployed as a dependency automatically (also possible to use external postgresql).
- Uses Message Brokers (rabbitMQ), which may be deployed as a dependency automatically.

This is how the setup looks after it's deployed to 'cfy-example' namespace (it's possible to have multiple replicas (pods) of cloudify manager):

![cfy-manager](../images/cfy-example.png)

## Prerequisites

- Docker installed
- Kubectl installed
- Helm installed
- Running Kubernetes cluster (View differences between cloud providers)
  - [EKS on AWS](./../examples/aws/README.md)
  - [AKS on Azure](./../examples/azure/README.md)
  - [GKE on GCP](./../examples/gcp/README.md)
- Sufficient Kubernetes node [Minimum Requirements](https://docs.cloudify.co/latest/install_maintain/installation/prerequisites/)
- Cloudify Premium valid license (for Premium version)

## How to create and deploy such a setup?

1. [Generate certificate as a secret in k8s.](#generate-certificates-and-add-as-secret-to-k8s)

2. [Deployment of Cloudify manager worker with dependencies.](#install-cloudify-manager-worker)

3. [(Optional) Ensure UI access to the manager upon installation](#optional-ensure-ui-access-to-the-manager-upon-installation)

4. [(Optional) Extra configuration options](#configuration-options-of-cloudify-manager-worker-valuesyaml)

5. [Troubleshooting](#troubleshoot)

6. [Uninstallation of helm charts](#uninstallation)

**You need to deploy DB and Message Broker before deploying Cloudify manager worker**

## Generate certificates and add as secret to k8s

**SSL certificate must be provided, to secure communications between cloudify manager and posrgress/rabbitmq**

- ca.crt (to sign other certificates)

- tls.key

- tls.crt

### Option 1: Automatically generate certificates by cert-manager component installed to kubernetes cluster

**Cert-manager (https://cert-manager.io) should be previously installed into your k8s cluster for that!**

This feature is disabled by default in helm chart, you can enable it if you add to your helm values file:

```yaml
tls:
  certManager:
    generate: true
```

**NOTE: Secrets, generated by cert-manager won't be removed automatically if you uninstall helm release and can be re-used later.**

### Option 2: Create certificates using the community cloudify manager docker container

```bash
$ docker pull cloudifyplatform/community-cloudify-manager-aio:latest
$ docker run --name cfy_manager_local -d --restart unless-stopped --tmpfs /run --tmpfs /run/lock cloudifyplatform/community-cloudify-manager-aio
```

Exec to the manager and generate certificates

```bash
$ docker exec -it cfy_manager_local bash

# NAMESPACE to which cloudify-manager deployed, must be changed accordingly
$ cfy_manager generate-test-cert -s 'cloudify-manager-worker.NAMESPACE.svc.cluster.local,rabbitmq.NAMESPACE.svc.cluster.local,postgres-postgresql.NAMESPACE.svc.cluster.local'
```

You can change the name of the created certificates (inside the container):

```bash
$ cd /root/.cloudify-test-ca
$ mv cloudify-manager-worker.helm-update.svc.cluster.local.crt tls.crt
$ mv cloudify-manager-worker.helm-update.svc.cluster.local.key ./tls.key
```

Exit the container and copy the certificates from the container to your working environment:

```bash
$ docker cp cfy_manager_local:/root/.cloudify-test-ca/. ./
```

Create secret in k8s from certificates:

```bash
$ kubectl create secret generic cfy-certs --from-file=./tls.crt --from-file=./tls.key --from-file=./ca.crt -n NAMESPACE
```

### Option 3: Use cert-manager component installed to kubernetes cluster (manually)

You need to deploy those manifests, which will generate cfy-certs secret eventually, you need to change NAMESPACE to your namespace before.
You can find this manifest in external folder - cert-issuer.yaml

```yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: cfy-ca
spec:
  secretName: cfy-ca-tls
  commonName: NAMESPACE.svc.cluster.local
  usages:
    - server auth
    - client auth
  isCA: true
  duration: "87660h"
  issuerRef:
    name: selfsigned-issuer
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: cfy-ca-issuer
spec:
  ca:
    secretName: cfy-ca-tls
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: cfy-cert
spec:
  secretName: cfy-certs
  isCA: false
  duration: "87660h"
  usages:
    - server auth
    - client auth
  dnsNames:
    - "postgres-postgresql.NAMESPACE.svc.cluster.local"
    - "rabbitmq.NAMESPACE.svc.cluster.local"
    - "cloudify-manager-worker.NAMESPACE.svc.cluster.local"
    - "postgres-postgresql"
    - "rabbitmq"
    - "cloudify-manager-worker"
  issuerRef:
    name: cfy-ca-issuer
```

Create a local copy of the cert-issuer.yaml and apply it to the namespace:

```bash
$ kubectl apply -f ./cert-issuer.yaml -n NAMESPACE
```

## Clone cloudify-helm repo

This step is necessary because the following steps will require files from this directory

- In case you don't have Git installed - https://github.com/git-guides/install-git

```bash
$ git clone https://github.com/cloudify-cosmo/cloudify-helm.git && cd cloudify-helm
```

## Install cloudify manager worker

### Create secret/configMap with premium license - required if using Cloudify premium version

Create license.yaml file and populate it with license data


```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cfy-license
  namespace: <NAMESPACE>
data:
  cfy_license.yaml: |
    license:
      capabilities: null
      cloudify_version: null
      customer_id: <CUSTOMER_ID>
      expiration_date: 12/31/2021
      license_edition: Premium
      trial: false
    signature: !!binary |
      <LICENSE_KEY>
```

Enable license in values file

- License name (metadata.name) must match the secretName in the values file

```yaml
license:
  secretName: cfy-license
```

Apply created config map:

```bash
$ kubectl apply -f license.yaml
```

### Add the cloudify-helm repo

Add the cloudify-helm chart repo or upgrade it

```bash
$ helm repo add cloudify-helm https://cloudify-cosmo.github.io/cloudify-helm
```

or

```bash
$ helm repo update cloudify-helm
```

**If you want to customize the values it's recommended to do so before installing the chart** - [see configuration options below](#configuration-options-of-cloudify-manager-worker-valuesyaml), and either way make sure to review the values file.

### Enable PostgreSQL and RabbitMQ deployment

For now deploy PostgreSQL and RabbitMQ as dependent subcharts disabled by default for backward compatibility, so for new deployment you need to enable them.

To do that please ensure you have following parameters in the values file:

```yaml
postgresql:
  deploy: true

rabbitmq:
  deploy: true
```

### (optional) If you want to use k8s secrets for store passwords

#### PostgreSQL initial password

Create k8s secret:

```bash
$ kubectl -n NAMESPACE create secret generic SECRET_NAME --from-literal=postgresql-password='POSTGRESQL_INIT_PASSWORD'
```

Update following parameters in your helm values file:

```yaml
db:
  serverExistingPasswordSecret: "SECRET_NAME"
  
postgresql:
  existingSecret: "SECRET_NAME"
```

#### PostgreSQL application connection password

Create k8s secret:

```bash
$ kubectl -n NAMESPACE create secret generic SECRET_NAME --from-literal=postgresql-cloudify-password='POSTGRESQL_CLOUDIFY_PASSWORD'
```

Update following parameters in your helm values file:

```yaml
db:
  cloudifyExistingPassword:
    secret: "SECRET_NAME"
```

#### RabitMQ password

Create k8s secret:

```bash
$ kubectl -n NAMESPACE create secret generic SECRET_NAME --from-literal=rabbitmq-password='RABBITMQ_PASSWORD'
```

Update following parameters in your helm values file:

```yaml
queue:
  existingPasswordSecret: "SECRET_NAME"
  
rabbitmq:
  auth:
    existingPasswordSecret: "SECRET_NAME"
```

#### Cloudify Manager worker admin password


Create k8s secret:

```bash
$ kubectl -n NAMESPACE create secret generic SECRET_NAME --from-literal=cfy-admin-password='CLOUDIFY_ADMIN_PASSWORD'
```

Update following parameters in your helm values file:

```yaml
config:
  security:
    existingAdminPassword:
      secret: "SECRET_NAME"
```

### (optional) Ensure UI access to the manager upon installation

### **[OPTION 1]**

Use ingress-controller (e.g. NGINX Ingress Controller - https://kubernetes.github.io/ingress-nginx/deploy/)

**HTTP**

- Modify Ingress section accordingly (see example):
  ```yaml
  ingress:
    enabled: true
    host: cloudify-manager.DOMAIN
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-body-size: 50m # use this annotation to allow upload of resources up to 50mb (e.g. plugins)
      # cert-manager.io/cluster-issuer: "letsencrypt-prod" # use this annotation to utilize an installed cert-manager
    tls:
      enabled: false
      secretName: cfy-secret-name
  ```
  **HTTPS - Pre-applied SSL Cert**
- Create SSL secret with tls certificate
  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: cfy-secret-name
    namespace: NAMESPACE
  data:
    tls.crt: SSL_TLS_CRT
    tls.key: SSL_TLS_KEY
  type: kubernetes.io/tls
  ```
- Modify Ingress section accordingly (see example):
  ```yaml
  ingress:
    enabled: true
    host: cloudify-manager.DOMAIN
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-body-size: 50m # use this annotation to allow upload of resources up to 50mb (e.g. plugins)
      # cert-manager.io/cluster-issuer: "letsencrypt-prod" # use this annotation to utilize an installed cert-manager
    tls:
      enabled: true
      secretName: cfy-secret-name
  ```
  **HTTPS - Certificate Manager**
- Use certificate manager (e.g. Let's Encrypt via cert-manager - https://cert-manager.io/docs/)
- Modify Ingress section accordingly (see example):
  ```yaml
  ingress:
    enabled: true
    host: cloudify-manager.DOMAIN
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-body-size: 50m # use this annotation to allow upload of resources up to 50mb (e.g. plugins)
      cert-manager.io/cluster-issuer: "<cluster-issuer-name>" # use this annotation to utilize an installed cert-manager
    tls:
      enabled: true
      secretName: cfy-secret-name
  **HTTP/HTTPS options will expose Cloudify Manager UI on a URL matching the `host` value**
  ```

### **[OPTION 2]**

Skip Ingress and expose the Cloudify Manager service using LoadBalancer.

To have a fixed URL, you must utilize a DNS service to route the LB URL (hostname) to the URL you want.

**HTTP**

For this method you need to edit the Service section to use the right type:

```yaml
service:
  host: cloudify-manager-worker
  type: LoadBalancer
  name: cloudify-manager-worker
  http:
    port: 80
  https:
    port: 443
  internal_rest:
    port: 53333
```

That will create a load balancer depending on your K8S infrastructure (e.g. EKS will create a Classic Load Balancer).

Also please add parameter **config.public_ip** with DNS name which you are going to configure for you Cloudify Manager  load balancer endpoint, for example:

```yaml
config:
  public_ip: cloudify-manager.example.com
```

**To get the hostname of the load balancer run:**

```bash
$ kubectl describe svc/cloudify-manager-worker -n NAMESPACE | grep Ingress
```

Then you can configure DNS record (ALIAS type), points to this load balancer hostname.

**The value of the ingress will be the UI URL of the Cloudify Manager.**

**HTTPS**

- To secure the site with SSL you can update the load balancer configuration to utilize an SSL Certificate

### After values are verified, install the manager worker chart

```bash
$ helm install cloudify-manager-worker cloudify-helm/cloudify-manager-worker --version 0.4.0 -f ./cloudify-manager-worker/values.yaml -n NAMESPACE
```

## Configuration options of cloudify-manager-worker values.yaml

{{ template "chart.valuesTable" . }}

Edit the values file in `./cloudify-manager-worker/values.yaml` according to your preferences:

### Upgrade cloudify manager worker

To upgrade cloudify manager use 'helm upgrade'.

For example to change to newer version (e.g. from 6.2.0 to 6.3.0 in this example),

Change image version in values.yaml:

Before:

```yaml
image:
  repository: cloudifyplatform/premium-cloudify-manager-worker
  tag: 6.2.0
```

After:

```yaml
image:
  repository: cloudifyplatform/premium-cloudify-manager-worker
  tag: 6.3.0
```

Run 'helm upgrade'

```bash
$ helm upgrade cloudify-manager-worker cloudify-helm/cloudify-manager-worker -f ./cloudify-manager-worker/values.yaml -n NAMESPACE
```

If DB schema was changed in newer version, needed migration will be running first on DB, then application will be restarted during upgrade - be patient, because it may take a couple of minutes.


#### Upgrading RabbitMQ

If as part of the helm upgrade the RabbitMQ node is upgraded (e.g. if the 
upgrade specifies a newer RabbitMQ image), the user will need to preserve 
the erlang cookie from the existing installation. It can be done as follows:

```bash
$ export RABBITMQ_ERLANG_COOKIE=$(kubectl get secret --namespace "michael-ns" rabbitmq -o jsonpath="{.data.rabbitmq-erlang-cookie}" | base64 --decode)
```
and then:
```bash
helm upgrade cloudify-manager-worker cloudify-helm/cloudify-manager-worker \
--set rabbitmq.auth.erlangCookie=$RABBITMQ_ERLANG_COOKIE
```


#### Upgrading to Cloudify manager worker 7.X [UNRELEASED]

In Cloudify Manager 7 we upgraded the version of PostgreSQL to 14 and of 
RabbitMQ to 3.10. 

In Cloudify-Helm we use the bitnami/postgresql chart 
(which deploys PostgreSQL version 11 for Cloudify Manager 6.X, also compatible 
with Cloudify Manager 7.X). The bitnami/postgresql chart does not support 
database version upgrade, so if the user likes to upgrade PostgreSQL they'll 
need to do so manually using the [pg_upgrade](https://www.postgresql.org/docs/current/pgupgrade.html) tool.

Additionally, coming from Cloudify Manager 6.X, the 
_populate_deployment_statuses_ and _migrate_pickle_to_json_ scripts must run 
on the data in the Manager's database in order to use it in the new version.
It is handled in the `config.after_bash` value.

Change the following in values.yaml:
```yaml
image:
  repository: cloudifyplatform/premium-cloudify-manager-worker
  tag: 7.0.0
...

rabbitmq:
  
  image:
    tag: 3.10.13-debian-11-r9
...

config:
  
  after_bash: "if [[ $(/opt/manager/env/bin/python --version) == *'3.10'* ]]; then opt/manager/env/bin/python /opt/mgmtworker/env/lib/python3.10/site-packages/cloudify_system_workflows/snapshots/populate_deployment_statuses.py; opt/manager/env/bin/python /opt/mgmtworker/env/lib/python3.10/site-packages/cloudify_system_workflows/snapshots/migrate_pickle_to_json.py; fi"
```
Alternatively these can be set directly through the `helm upgrade` command 
(notice the `--set` of the erlang cookie, see above):

```bash
$ helm upgrade cloudify-manager-worker cloudify-helm/cloudify-manager-worker \
--reuse-values --set image.tag=7.0.0 --set rabbitmq.image.tag='3.10.13-debian-11-r9' \
--set config.after_bash="if [[ \$(/opt/manager/env/bin/python --version) == *'3.10'* ]]; then opt/manager/env/bin/python /opt/mgmtworker/env/lib/python3.10/site-packages/cloudify_system_workflows/snapshots/populate_deployment_statuses.py; opt/manager/env/bin/python /opt/mgmtworker/env/lib/python3.10/site-packages/cloudify_system_workflows/snapshots/migrate_pickle_to_json.py; fi" \
--set rabbitmq.auth.erlangCookie=$RABBITMQ_ERLANG_COOKIE -n NAMESPACE
```


### Image:

```yaml
image:
  repository: "cloudifyplatform/premium-cloudify-manager-worker"
  tag: "6.3.0"
  pullPolicy: IfNotPresent
```

### DB - postgreSQL:

```yaml
db:
  useExternalDB: false # when switched to true, it will take the FQDN for the pgsql database in host, and require CA cert in secret inputs under TLS section
  postgresqlSslClientVerification: true
  host: postgres-postgresql
  cloudify_db_name: "cloudify_db"
  cloudify_username: "cloudify"
  cloudify_password: "cloudify"
  server_db_name: "postgres"
  server_username: "postgres"
  server_password: "cfy_test_pass"
```

### Message Broker - rabbitmq:

```yaml
queue:
  host: rabbitmq
  username: "cfy_user"
  password: "cfy_test_pass"
```

### Service:

[See customization example above](#option-2)

```yaml
service:
  host: cloudify-manager-worker
  type: ClusterIP
  name: cloudify-manager-worker
  http:
    port: 80
  https:
    port: 443
  internal_rest:
    port: 53333
```

### node selector - select on which nodes cloudify manager may run:

```yaml
nodeSelector: {}
# nodeSelector:
#   nodeType: onDemand
```

### Secrets for TLS/SSL certificates

```yaml
tls:
  # certificates as a secret, to secure communications between cloudify manager and postgress|rabbitmq
  secretName: cfy-certs
  # Parameters for PostgreSQL SSL certificates, required for external postgresql database only
  pgsqlSslSecretName: pgsql-external-cert # k8s secret name with psql ssl certs
  pgsqlSslCaName: postgres_ca.crt # subPath name for ssl CA cert in k8s secret
  pgsqlSslCertName: '' # subPath name for ssl cert in k8s secret, isn't required
  pgsqlSslKeyName: '' # subPath name for ssl key in k8s secret, isn't required
```

### Additional secrets to mount onto cloudify-manager-worker pod

In case you need to mount additional secrets to the pod (e.g. self-signed certificate for 3rd party connection)
For each secret you need to define its mounts. It's possible to mounts several files under one secret definition using different subPaths
See below or example in the values file.

```yaml
additionalSecrets:
  - name: secretName
    mounts:
      - mountPath: /mnt/cloudify-data/ssl/secretName.crt
        subPath: secretName.crt
      - mountPath: /mnt/cloudify-data/ssl/secretName.key
        subPath: secretName.key
```

### resources requests and limits:

```yaml
resources:
  requests:
    memory: 0.5Gi
    cpu: 0.5
```

### Persistent volume size for EBS/EFS:

If using multiple replicas (High availability), NFS like Storage like EFS must be used.
For more details see links to different cloud providers [here](#prerequisites)

```yaml
volume:
  storage_class: "efs"
  access_mode: "ReadWriteMany"
  size: "3Gi"
```

If using one replicas, you can use EBS (gp2) for example, **gp2 is default**:

```yaml
volume:
  storage_class: "gp2"
  access_mode: "ReadWriteOnce"
  size: "3Gi"
```

### readiness probe may be enabled/disabled

```yaml
readinessProbe:
  enabled: true
  port: 80
  path: /console
  initialDelaySeconds: 10
```

**NOTE:** If you need to deploy Cloudify Manager in high availability mode with **2** replicas (config.replicas=2), and your k8s cluster version **< 1.25** please set value for parameter **readinessProbe.initialDelaySeconds** to **120** for avoid issues with second replica start.

### Config

You can delay start of cfy manager / install all plugins / disable security (not recommended)...

```yaml
config:
  start_delay: 0
  # Multiple replicas works only with EFS(NFS) volume
  replicas: 1
  install_plugins: false
  cli_local_profile_host_name: localhost
  security:
    ssl_enabled: false
    admin_password: admin
  tls_cert_path: /mnt/cloudify-data/ssl/tls.crt
  tls_key_path: /mnt/cloudify-data/ssl/tls.key
  ca_cert_path: /mnt/cloudify-data/ssl/ca.crt
```

### Ingress

You may enable ingress-nginx and generate automatically cert if you have ingress-nginx / cert-manager installed (e.g. using nginx with existing ssl secret) - [See above for more details](#option-1)

```yaml
ingress:
  enabled: false
  host: cloudify-manager.app.cloudify.co
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 50m # use this annotation to allow upload of resources up to 50mb (e.g. plugins)
    # cert-manager.io/cluster-issuer: "letsencrypt-prod" # use this annotation to utilize an installed cert-manager
  tls:
    enabled: false
    secretName: cfy-secret-name
```

### Create secret for okta-license - required if using Okta/SSO

First obtain the okta/sso server certificate and store it in a file for exanple okta_certificate.pem

```bash
echo -n '-----BEGIN CERTIFICATE-----
MIIDXDCCAkSgAwIBAgIJAKwO13ndNBPjMA0GCSqGSIb3DQEBCwUAMCkxJzAlBgNV
BAMMHkNsb3VkaWZ5IGdlbmVyYXRlZCBjZXJ0aWZpY2F0ZTAeFw0yMTA2MTQxMjMx
NDZaFw0zMTA2MTIxMjMxNDZaMCYxJDAiBgNVBAMMG2lwLTEwLTEwLTQtMTE2LmVj
-----END CERTIFICATE-----'  > ./okta_certificate.pem
```

then let's create the okta-license secret in the NAMESPACE

```bash
kubectl create secret generic okta-license --from-file=./okta_certificate.pem -n NAMESPACE
```

## Troubleshoot

Some common use cases:

### License is not uploaded correctly upon installation

[deprecated] This might happen if the English convention of licence/license is not alligned across the values (name of the value and its value), or across the license/licence configMap.

Since v0.4.1 `licence` convention is not supported. please use `license`.

The [StatefulSet](./templates/statefulset.yaml) accepts a [secret/configMap](#create-secret/configmap-with-premium-license---required-if-using-cloudify-premium-version) with the `data` value of this syntax `cfy_license.yaml`

After ensuring the above, try to reinstall the worker chart

- Workaround for this issue would be to manually upload the license after the manager installation through the UI after logging in or via the [CLI](https://docs.cloudify.co/latest/cli/maint_cli/license/).

### Cloudify Manager installation succeded but I can't reach the UI

Please see [above](#optional-ensure-ui-access-to-the-manager-upon-installation)

If you already installed the chart, update the values accordingly and run:

```bash
$ helm upgrade cloudify-manager-worker cloudify-helm/cloudify-manager-worker -f <path-to-values.yaml-file> -n NAMESPACE
```

### I had to reinstall the worker chart and now it fails on installation

This might happen due to inter-communications between the components in the different pods, a work around for that would be to delete the postgresql (has a PersistentVolume) and the rabbitmq pods, which will trigger a restart for them.

```bash
$ kubectl delete pod postgres-postgresql-0 -n NAMESPACE
$ kubectl delete pod rabbitmq-0 -n NAMESPACE
```

Then try reinstalling the worker chart.

### Can't find the help you need here?

Feel free to open an [issue](https://github.com/cloudify-cosmo/cloudify-helm/issues) in the helm chart GitHub page, or [contact us](https://cloudify.co/contact/) through our website.

## Uninstallation

Uninstall helm release:

```bash
$ helm uninstall cloudify-manager-worker -n NAMESPACE
```

If you want to remove Persistent Volume Claims (for example if you are going to reinstall stack without data saving):

```
kubectl --namespace NAMESPACE delete persistentvolumeclaims cfy-worker-pvc data-postgres-postgresql-0 data-rabbitmq-0
```

To clean the supporting files:

```bash
$ kubectl delete secret cfy-certs -n NAMESPACE
$ kubectl delete secret cfy-ca-tls -n NAMESPACE
$ kubectl delete configmap cfy-license -n NAMESPACE
```

[Jump to Top](#cloudify-manager-worker-helm-chart--premium-version)
